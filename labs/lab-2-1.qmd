---
title: "Lab 2-1 - Predicting transit accessibility with census data"
author: "Hamish Gibbs"
institute:
  - Network Science Institute, Northeastern University
  - NETS 7983 Computational Urban Science
  - Last updated `r Sys.Date()`
format: html
code-line-numbers: false
execute:
  echo: true
#  cache: true
  warning: false
  messages: false
editor: visual
logo: ./img/sunlab.png
footer: "CUS, Northeastern Spring 2025"
link-external-icon: true
link-external-newwindow: true
---

## Objective

In this practical, we will conduct an analysis of transit accessibility, and use this analysis as a gateway to explore the underlying social characteristics measured by the census.

The aim of this practical is to learn how to:

-   Retrieve, process, and analyze census data for different geographies / variables.
-   Combine geo-referenced census data with other spatial features (from Open Street Map) for analysis.
-   Compute a measure of public transit accessibility and assess how well this measure is predicted by census variables.
-   Conduct a *social area analysis* using dimensionality reduction (PCA) to identify the factors which differentiate census variables.

## Before starting:

1.  Ensure you have access to the `stella` server:

2.  [Request a U.S. Census Data API Key](https://api.census.gov/data/key_signup.html)

3.  Load required packages

    ```{r}
    library(tidycensus)
    library(tidyverse)
    library(tigris)
    library(sf)
    library(osmdata)
    library(leaflet)
    library(arrow)
    library(corrplot)
    library(factoextra)
    ```

4.  Configure the API key in your R environment

    ``` r
    Sys.setenv(CENSUS_API_KEY = "YOUR_API_KEY")
    ```

5.  Then call the `tidycensus::census_api_key` function:

    ```{r}
    census_api_key(Sys.getenv("CENSUS_API_KEY"), install = TRUE, overwrite=TRUE)
    ```

## Loading Census Data With `tidycensus`

To recap, the `tidycensus` package gives us a simple way to retrieve census data for a given variable and set of geographic units.

*Note: in this practical, we will use data from the American Community Survey (ACS) because of the greater number of variables. To do this, we will use the `tidyverse::get_acs` function. The decennial census is also available with `tidyverse::get_decennial`.*

Lets start by retrieving median household income in US states. For a list of the different variables available from the ACS, a good place to start is the [ACS Variable Explorer](https://www.census.gov/programs-surveys/acs/guidance/subjects.html). If you know what you are looking for, you can use the search bar to find the variable code(s).

```{r}
medincome <- get_acs(geography = "state", 
                   variables = "B19013_001", 
                   year = 2021,
                   progress = FALSE)

head(medincome)
```

## Exercise 1

-   Try downloading other variables using codes you find with the [ACS Variable Explorer](https://www.census.gov/programs-surveys/acs/guidance/subjects.html). Are cross-tabulations of multiple variables in the same format as univariate tables?

-   Inspect all of the variables available in the 2021 ACS. `tidycensus` has a helpful function for this: `load_variables(2021, "acs5", cache = TRUE)` .

-   The beginning of the variable code indicates the table. What does the suffix `_001` in the code block above indicate?

## Mapping Census Data

`tidycensus` integrates with `sf` to return spatial boundaries for geographic units. We can add spatial boundaries using `geometry = TRUE`.

```{r}
medincome <- get_acs(geography = "state", 
                     variables = "B19013_001", 
                     year = 2021,
                     geometry=TRUE,
                     progress=FALSE)

ggplot(medincome) + 
    geom_sf(aes(fill=estimate))
```

We can simplify the plot by restricting to states in the continental US.

```{r}
medincome_continental_usa <- medincome %>% 
    filter(!NAME %in% c("Alaska", "Hawaii", "Guam", "Puerto Rico"))

ggplot(medincome_continental_usa) + 
    geom_sf(aes(fill=estimate))
```

Remember the hierarchy of US statistical geographies? You can download data for any of these geographies with `tidycensus`.

![](https://walker-data.com/umich-workshop-2022/intro-2020-census/img/census_diagram.png)

See the list of available geographies [here](https://walker-data.com/tidycensus/articles/basic-usage.html#geography-in-tidycensus).

We can also filter for some higher-level geographies. Lets get median household income for Census Block Groups in Massachusetts.

```{r}
medincome_ma <- get_acs(
  geography = "cbg",
  variables = "B19013_001",
  survey = "acs5",
  year = 2021,
  state = "MA",
  geometry = TRUE,
  progress = FALSE
)

head(medincome_ma)
```

In this practical, we only want data for Boston, so we will have to filter for CBGs within the boundary of Boston. We can get this boundary from `tigris`, an R package that provides access to Census geographic boundaries and is closely coupled to `tidycensus`. See [Census geographic data and applications in R](https://walker-data.com/census-r/census-geographic-data-and-applications-in-r.html) for more information.

```{r}
boston_area_towns <- c("Boston", "Somerville", "Cambridge", "Brookline", "Revere", "Malden", "Everett", "Medford", "Chelsea", "Winthrop Town")
boston_boundary <- places(state = "MA", cb = TRUE, year=2021,progress=FALSE) %>%
  filter(NAME %in% boston_area_towns)
```

## Exercise 2

-   Using what you learned in lab-1-1, check that the Boston boundary is correct by plotting it on an interactive map using `leaflet`.

## Restricting Census data to our area of interest

Now, using `st_filter` from the `sf` package, we can filter for CBGs within Boston.

```{r}
medincome_boston <- medincome_ma %>%
  st_filter(boston_boundary)

head(medincome_boston)
```

We can then make a Choropleth map of the income in CBGs in Boston.

```{r}
ggplot(medincome_boston) +
  geom_sf(aes(fill = estimate), color = NA)
```

## Exercise 3

-   What is causing the missing values for some of these areas? *Tip: Removing CBGs with `NA` values and plotting with `leaflet` may give some indication.*

## Sidenote: using data visualizations to communicate your findings

Data visualizations are the best tool you have for conveying the results of your analysis. It is well worth spending a bit of effort to (1) **think through how to best display your data**, (2) **take a extra time to polish your visualization**, (3) **be prepared to make and re-make your visualization as things change**. Making a compelling data visualization is often a question of choosing which dimensions in your data that are most important for the message you are trying to convey, then finding a type of plot that can represent them clearly. There is some inspiration for different types of plots in the [R Graph Gallery](https://r-graph-gallery.com/).

Things to keep in mind:

-   Are your axis labels human readable?

-   Are you maximizing the *data-to-ink ratio?*

-   Does your plot have an appropriate theme? Grid-lines (in default ggplot theme) are often not relevant or need to be reduced.

-   Have you considered how you are using color? Common mistakes include: colors over-emphasizing minor variations in your data, diverging color scales used for non-diverging data, forgetting accessibility for color blind people.

-   (Sometimes) Do you have a **declarative title** conveying the main message of your visualization?

Here is a cleaned up version of the map from above.

```{r}
ggplot() +
  geom_sf(data = medincome_boston, aes(fill = estimate), color = NA, alpha = 0.8) +
  scale_fill_viridis_c(option = "magma", direction = 1, name = "MHI", label = function(x){paste0("$", scales::comma(x))}) + 
  theme_void() +
  labs(
    title = "Median Household Income (2021)",
    subtitle = "Block Groups in Boston"
  ) + 
theme(legend.position = c(0.8, 0.2))
```

## Exercise 4

-   List or try to add a few more features to improve this visualization.

## Combining census data with OSM

Now, the aim of this practical is to understand the factors which predict public transit accessibility in Boston.

We will follow this methodology:

1.  Download a range of ACS variables.
2.  Download Public Transit Stops in Boston from Open Street Map.
3.  Compute the "accessibility" of transit stations from CBGs.
4.  Explore which census variables predict transit station accessibility.

## Retrieving ACS Variables

Now, lets retrieve a few more variables from the ACS. Because combining ACS variables can be complicated (for example, we might want broad age categories, not one year bands), we have pre-processed a few key variables into an easier-to-use format. You can find this in the file `/data/CUS/labs/2/14460_acs_2021_filtered_boston.parquet`.

```{r}
census <- read_parquet("/data/CUS/labs/2/14460_acs_2021_filtered_boston.parquet")
colnames(census)
```

## Exercise 5

-   Take a moment to re-construct one of the variables (like `age.u1825`) in this data frame using `tidycensus` and the [ACS Variable Explorer](https://www.census.gov/programs-surveys/acs/guidance/subjects.html). How many variables do you need to combine?

Because this pre-processed `census` data doesn't have any associated geography, we need to download and attach geographies using the `tigris` package.

```{r}
block_groups <- block_groups(state = "25", year = 2021, class = "sf",progress=FALSE)
census <- census %>% 
    left_join(block_groups %>% select(GEOID, geometry), by = c("GEOID")) %>% 
    st_as_sf()
census
```

## Retrieving OSM data

Now that we have some nicely formatted census data, we can move on to downloading our transit stations from OSM.

We have already loaded ACS data for CBGs in Boston. Now we need to retrieve public transit stations from OSM. We can do this using the `osmdata` R package (as we did in last week's practical).

```{r}
boston_bb <- getbb("Boston, Massachusetts")
public_transit_pts <- opq(bbox = boston_bb) %>%
  add_osm_feature(key = "public_transport", value = "station") %>%
  osmdata_sf()

public_transit_pts <- public_transit_pts$osm_points
head(public_transit_pts)
```

Lets display the data in an interactive map that will let us check that we have downloaded the correct features.

```{r}
leaflet(public_transit_pts) %>%
  addProviderTiles(provider=providers$CartoDB.Positron) %>%
  addCircleMarkers(
    label = ~name,
    radius = 3,
    stroke = FALSE,
    fillOpacity = 0.8,
    color = "blue"
  )
```

It looks like this data has some duplicated stations, and isn't restricted to T-stops. This is a common problem in OSM data, which is crowd-sourced from volunteers.

Let's see if we can filter the data to select only T stops. First, we can check the unique values in the station column.

```{r}
public_transit_pts %>% pull(station) %>% unique()
```

Then, lets filter for "subway" and "light_rail" stations.

```{r}
t_stops <- public_transit_pts %>%
  filter(station %in% c("subway", "light_rail"))

leaflet(t_stops) %>%
  addProviderTiles(provider=providers$CartoDB.Positron) %>%
  addCircleMarkers(
    label = ~name,
    radius = 3,
    stroke = FALSE,
    fillOpacity = 0.8,
    color = "blue"
  )
```

This looks pretty good!

## Exercise 6

-   Take a moment to explore OSM data. What other features are accessible? Can you find features for other forms of public transit? Can you identify any limitations cause by the crowd-sourced nature of the data?

    -   For a list of all of the features available from Open Street Map, see [here](https://wiki.openstreetmap.org/wiki/Map_features).

## Measuring transit accessibility

Now that we have data on census variables in geographic areas and the location of public transit stations, we can move on to calculating a measure of public transit accessibility.

Our measure will be based on the assumption of *proximity-based accessibility* and *residential anchoring*. This means that the proximity of an individual's home to a transit station (in terms of geometric distance) is indicative of higher / lower public transit accessibility.

## Exercise 7

-   Take a moment to consider the assumptions underlying the ideas of *proximity-based accessibility* and *residential anchoring.* Could large-scale behavioral data shed more light on people's true patterns of behavior in the city?

-   Why do you think that these ideas have been widely adopted in traditional urban studies (in transit accessibility studies like this one, as well as concepts such as urban food deserts)?

-   What other types of accessibility might better represent people's actual ability to use public transportation?

## Computing a measure of transit accessibility

In order to compute our accessibility measure, we can use the `st_distance` function to calculate the distance from each Block Group to the nearest transit station.

To make sure our distance calculation is accurate, we need to transform our data into a projected coordinate system (which preserves distance). Because our study area is focused on Boston, we can use the [NAD83 / Massachusetts Mainland projection](https://epsg.io/26986) (EPSG:26986).

### **Sidenote: Understanding Projected Coordinate Systems**

**Geographic coordinate systems** like [WGS84](https://epsg.io/4326) (EPSG:4326), commonly used in latitude/longitude data represent locations on the Earth's surface using angular measurements. While suitable for global mapping, they aren't ideal for precise distance or area calculations because the Earth is curved.

**Projected coordinate systems**, such as the one we're using, transform geographic data onto a flat surface. This preserves specific properties, such as distance or area, making it more accurate for spatial operations like buffering or intersection analysis within a localized region.

By re-projecting our data to a system measured in meters, we ensure more accurate distance calculations between census block groups and transit stations.

```{r}
t_stops <- t_stops %>% st_transform(26986)
census <- census %>% st_transform(26986)
```

Now we can compute the distance from each CBG to the nearest transit station.

```{r}
distances <- st_distance(census, t_stops)
census$nearest_t_stop_dist <- apply(distances, 1, min)
census %>% select(GEOID, nearest_t_stop_dist)
```

*Tip: if you don't understand an expression like this (or any of the more complicated expressions that have come before), please take a moment to break the code down piece-by-piece. For example, try inspecting the `distances`* variable, or running only the expression `apply(distances, 1, min)` in the R console.

## Exercise 8

-   Make a map (static or interactive) of the `nearest_t_stop_dist` distance variable.
-   Add the `t_stops` to this map, overlaid on top of the census block groups.

## Predicting transit accessibility

Now we have a measure of public transit accessibility for census block groups in Boston. This means that we can ask the question: *what census variables predict the accessibility of transit stations?*

We can start with a simple approach: a linear regression of `our accessibility measure ~ block group characteristics`.

The first step is to compose a formula for this regression. To do this efficiently, I'm using the `as.formula` function from base R. Again, if you are new to R, take a moment to break down this expression piece-by-piece to understand how each component works. Basically, I want to have my `nearest_t_stop_dist` on the LHS of the equation, and all other variables (except `geometry` and `GEOID` on the RHS.

```{r}
predictors <- setdiff(names(census %>% select(-GEOID) %>% st_drop_geometry()), "nearest_t_stop_dist")
lm_formula <- as.formula(paste("nearest_t_stop_dist ~", paste(predictors, collapse = " + ")))
print(lm_formula)
```

Now that we have the formula for our regression, we can create our model and inspect the results.

```{r}
model <- lm(lm_formula, data = census %>% select(-GEOID) %>% st_drop_geometry())
summary(model)
```

## Exercise 9

-   Try to interpret these model results.

    -   Which variables are *positively* associated with `nearest_t_stop_dist`?

    -   Which variables are *negatively* associated?

    -   Why are some estimates `NA`?

    -   What does this indicate about the quantity of information contained in certain census variables?

    -   How many variables have any statistical significance? Which are they?

    -   What is the overall power of census variables to predict `nearest_t_stop_dist`?

## Multicollinearity of census variables

Our regression results raise a number of interesting questions: why is the overall predictability of `nearest_t_stop_dist` relatively low? Are all variables contributing unique information to our regression model? The answer seems to be no (because of the presence of `NA` coefficient estimates in our model).

To understand how differentiated our census variables are from one another, we can create a cross-correlation plot using the `corrplot` function. This shows the correlation coefficient of each census variable against all others. A few variables contain missing values which we will drop for convenience.

```{r}
corrplot(cor(census %>% select(-GEOID, -nearest_t_stop_dist) %>% st_drop_geometry() %>% drop_na()), method="number")
```

## Exercise 10

-   Interpret the results of `corrplot` above. What variables seem to be providing unique information? What groups of variables seem to be correlated with one another?

-   Explore the [`corrplot`](https://cran.r-project.org/web/packages/corrplot/vignettes/corrplot-intro.html) package. How else could we assess multicollinearity in our variables?

## Introduction: Social Area Analysis

The strong correlation between some of our variables raises the question of *what social characteristics* are differentiating census block groups from one another.

This question is a well established area of research in human geography and sociology, called Social Area Analysis, where researchers seek to understand the *latent* or *underlying* social characteristics driving differences between locations. This involves using dimensionality reduction techniques like Principal Component Analysis (PCA) to collapse a large number of census or other social variables into a smaller number of components.

Existing literature often identifies three recurring themes—socioeconomic status, household/family structure, and race/ethnicity—as the primary latent dimensions of social differentiation in U.S. Census data.

![](img/social_area_analysis_factors.png)

## Dimensionality reduction of census variables

Using the same census variables as we did in our analysis of transit accessibility, we can perform a Social Area Analysis and see whether our results align with the existing literature.

**Sidenote:** R has a number of packages that make it very easy to create complex visualizations. One of them is [`factoextra`](https://rpkgs.datanovia.com/factoextra/) which is great for visualizing the results of Principal Component and Clustering analyses.

First, lets run a PCA on our census variables. Note that I am removing the `GEOID`, `nearest_t_stop_dist`, and `geometry` columns because we are only interested in the census variables. Again, a few variables contain missing values which we will drop for convenience.

```{r}
pca <- prcomp(census %>% select(-GEOID, -nearest_t_stop_dist) %>% st_drop_geometry() %>% drop_na(), scale. = TRUE)
```

Now we can plot the amount of variation in the original dataset captured by each principal component.

```{r}
fviz_eig(pca, addlabels = TRUE, ylim = c(0, 50), ncp=Inf)
```

It looks like three principal components explain \~64% of variation in our census variables. This is a major dimensionality reduction. Moreover, the first component is by far the most important for explaining differences between census block groups. Now we can dig in further to try to understand what these components are capturing in the census data.

Now lets look at the Biplot, which shows the relationship between our original variables and the first two principal components we have identified.

```{r}
fviz_pca_biplot(pca, repel = TRUE, col.var = "blue", col.ind = NA)
```

## Exercise 11

-   Variables pointing in the same direction indicate correlated variables. What variables seem to be correlated with one another?

-   What variables seem to be driving the top three most important principal components? *Hint: these variables will be groups of longer vectors (arrows) pointing in similar directions.*

A final way we can analyze our PCA results is to look at the correlation between our original variables and the principal components. This can indicate which variables are playing a role in defining different components.

```{r}
cor_matrix <- cor(census %>% select(-GEOID, -nearest_t_stop_dist) %>% st_drop_geometry() %>% drop_na(), pca$x)
corrplot(cor_matrix, method = "color", is.corr = FALSE, addCoef.col = "black")
```

## Exercise 12

-   What variables are correlated (positively or negatively) with the first three principal components?

-   If the first three principal components have identified the principle axes of differentiation in our census data, how do these results compare to the table based on existing literature above?

## Visualizing key social factors

A final way that we can look at the principal components identified in our Social Area Analysis is by mapping them. To do this, we will have to combine the PCA results with our original geographic data.

```{r}
top_3_components <- as.data.frame(pca$x[, 1:3])

pca_with_geoid <- cbind(census %>% drop_na() %>% pull(GEOID), top_3_components)
colnames(pca_with_geoid)[1] <- "GEOID"
census_pca <- census %>%
  left_join(pca_with_geoid, by = "GEOID") 
```

## Exercise 13

-   Using leaflet, make three different maps of `PC1`, `PC2`, and `PC3`. Which of these principal components show a spatial pattern? Do any of them *not* show a spatial pattern?

## Exercise 14

-   In today's lecture, we discussed the importance of considering the spatial units which define your analysis, and the challenge of the Modifiable Areal Unit Problem, where results can be sensitive to the choice of spatial aggregation. Re-conduct this analysis using higher-level statistical geographies (either Census Tract or County).

    -   Do the associations between transit accessibility and different demographic variables remain the same?

    -   Does the dimensionality reduction identify similar social factors differentiating areas?

## Closing thoughts

This practical is intended to show you some of the strengths and limitations of census data for understanding urban phenomena. Although census data are a valuable source of *big data* in cities, they capture only a limited subset of human behavior. Take a moment to consider how large-scale behavioral data could shed more light on urban dynamics, and how this data can complement the census.

The finding that a majority of variation in census variables can be captured by a small number of components (3) is another important takeaway. Consider the reasons for the high level of multicollinearity in census variables, and what processes (social, behavioral, environmental, historical) might contribute to this pattern.
